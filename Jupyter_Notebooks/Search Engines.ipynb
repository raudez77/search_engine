{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Libraries \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from feature_engine.encoding import OneHotEncoder\n",
    "import pandas \n",
    "import numpy \n",
    "import nltk\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Pre-processing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading Data set\n",
    "DATA = r'..\\search_engines\\data_base\\bbc-news-data.csv'\n",
    "df = pandas.read_csv(DATA, sep = '\\t')\n",
    "df.category.value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classes and functions\n",
    "\n",
    "- The Data frame only contains content and title.\n",
    "- I used bert to extract the top 3 phrases from each news.\n",
    "- I did this because I want to write another program to extract keywords from documents , send them to a data base \n",
    "re-train the model and follow thi cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Process keywords \n",
    "class extract_keywords_keybert (sklearn.base.BaseEstimator, \n",
    "                        sklearn.base.TransformerMixin):\n",
    "    \n",
    "    \"\"\"KeyBERT Must be imported, as dependence\"\"\"\n",
    "    \n",
    "    def __init__(self, variable:list, extractor ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if not isinstance(variable, list):\n",
    "            raise ValueError (\"feature must be a list\")\n",
    "            \n",
    "        self.variable = variable\n",
    "        self.key_extractor = extractor\n",
    "        self.all_key_words = []\n",
    "            \n",
    "    def fit (self, X :pandas.DataFrame , y:pandas.Series = None):\n",
    "        return self \n",
    "    \n",
    "    def transform( self, X:pandas.DataFrame) -> pandas.DataFrame:\n",
    "        \n",
    "        def extracting_keywords (row:str) -> list:\n",
    "            \"\"\"Initiate tranformer , extract keywords into a list\"\"\"\n",
    "            \n",
    "            keywords_ = (self.key_extractor.extract_keywords(docs=row,\n",
    "                                                             keyphrase_ngram_range = (1,2),\n",
    "                                                             stop_words ='english',\n",
    "                                                             top_n = 3,\n",
    "                                                             use_mmr = True,\n",
    "                                                             diversity = .6))\n",
    "            \n",
    "            keywords_ = [word[0] for word in keywords_]\n",
    "            return keywords_\n",
    "            \n",
    "        \n",
    "        # create a copy \n",
    "        X = X.copy()\n",
    "        \n",
    "        for col in self.variable:\n",
    "            X[col+str(\"_t\")] = X[col].apply(lambda row : extracting_keywords(row))\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Write a fuction for one hot enconding \n",
    "Categories = ['category']\n",
    "Title = ['title']\n",
    "Content = ['content']\n",
    "\n",
    "# Extract Keywords\n",
    "from keybert import KeyBERT\n",
    "kw_model = KeyBERT()\n",
    "\n",
    "# ----> Creating Pipelines \n",
    "Pipeline_ = sklearn.pipeline.Pipeline([\n",
    "    \n",
    "    # === OneHotEncoder ===\n",
    "    (\"OneHotEncoder\",\n",
    "      OneHotEncoder(variables=Categories)),\n",
    "    \n",
    "    # === Keywords Extraction ===\n",
    "    (\"keywords_title_extraction\",\n",
    "     extract_keywords_keybert(Title,kw_model)),\n",
    "    \n",
    "    (\"keywords_content_extraction\",\n",
    "     extract_keywords_keybert(Content,kw_model))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mgarcia4\\Desktop\\Logitec-Search-Engine\\search_engine\\Jupyter_Notebooks\\Search Engines.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mgarcia4/Desktop/Logitec-Search-Engine/search_engine/Jupyter_Notebooks/Search%20Engines.ipynb#ch0000006?line=0'>1</a>\u001b[0m engine \u001b[39m=\u001b[39m Pipeline_\u001b[39m.\u001b[39mfit_transform(df)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mgarcia4/Desktop/Logitec-Search-Engine/search_engine/Jupyter_Notebooks/Search%20Engines.ipynb#ch0000006?line=1'>2</a>\u001b[0m engine\u001b[39m.\u001b[39mhead(\u001b[39m5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Pipeline_' is not defined"
     ]
    }
   ],
   "source": [
    "engine = Pipeline_.fit_transform(df)\n",
    "engine.head(5)\n",
    "# engine.to_csv(\"anynameyouwant.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Search Engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def extract_best_indices(m, topk, mask=None):\n",
    "    \"\"\"\n",
    "    Use sum of the cosine distance over all tokens.\n",
    "    m (np.array): cos matrix of shape (nb_in_tokens, nb_dict_tokens)\n",
    "    topk (int): number of indices to return (from high to lowest in order)\n",
    "    \"\"\"\n",
    "    # return the sum on all tokens of cosinus for each sentence\n",
    "    if len(m.shape) > 1:\n",
    "        cos_sim = numpy.mean(m, axis=0) \n",
    "    else: \n",
    "        cos_sim = m\n",
    "    index = numpy.argsort(cos_sim)[::-1] # from highest idx to smallest score \n",
    "    if mask is not None:\n",
    "        assert mask.shape == m.shape\n",
    "        mask = mask[index]\n",
    "    else:\n",
    "        mask = numpy.ones(len(cos_sim))\n",
    "    mask = numpy.logical_or(cos_sim[index] != 0, mask) #eliminate 0 cosine distance\n",
    "    best_index = index[mask][:topk]  \n",
    "    return best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category_business</th>\n",
       "      <th>category_entertainment</th>\n",
       "      <th>category_politics</th>\n",
       "      <th>category_sport</th>\n",
       "      <th>category_tech</th>\n",
       "      <th>title_t</th>\n",
       "      <th>content_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>2223</td>\n",
       "      <td>400.txt</td>\n",
       "      <td>US cyber security chief resigns</td>\n",
       "      <td>The man making sure US computer networks are ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['chief resigns', 'cyber security', 'security ...</td>\n",
       "      <td>['department cybersecurity', 'yoran took', 'ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>2224</td>\n",
       "      <td>401.txt</td>\n",
       "      <td>Losing yourself in online gaming</td>\n",
       "      <td>Online role playing games are time-consuming,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['online gaming', 'losing', 'losing online']</td>\n",
       "      <td>['gaming addiction', 'huddersfield scare', 'on...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 filename                             title  \\\n",
       "2223        2223  400.txt   US cyber security chief resigns   \n",
       "2224        2224  401.txt  Losing yourself in online gaming   \n",
       "\n",
       "                                                content  category_business  \\\n",
       "2223   The man making sure US computer networks are ...                  0   \n",
       "2224   Online role playing games are time-consuming,...                  0   \n",
       "\n",
       "      category_entertainment  category_politics  category_sport  \\\n",
       "2223                       0                  0               0   \n",
       "2224                       0                  0               0   \n",
       "\n",
       "      category_tech                                            title_t  \\\n",
       "2223              1  ['chief resigns', 'cyber security', 'security ...   \n",
       "2224              1       ['online gaming', 'losing', 'losing online']   \n",
       "\n",
       "                                              content_t  \n",
       "2223  ['department cybersecurity', 'yoran took', 'ma...  \n",
       "2224  ['gaming addiction', 'huddersfield scare', 'on...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 Connect to my Database (skip because it is in my data frame)\n",
    "NDATA = r'..\\search_engines\\data_base\\bbc-tokenized.csv'\n",
    "database = pandas.read_csv(NDATA, sep =',')\n",
    "database.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Search by Keywords only\n",
    "\n",
    "- This is search engine is basically a filter it search the words that you provide and find the articles link to those keywords\n",
    "which means that it depends on how well the \"keywords\" describe the artical and also the number of keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def search_bar(keywords:str,category:str) -> pandas.DataFrame:\n",
    "    \"\"\"filter based on differnet keywords and categories\n",
    "    Arg: \n",
    "        keywords: must be string \n",
    "        category: must be string (optional)\n",
    "        \n",
    "    Return:\n",
    "        DataFrame\"\"\"\n",
    "\n",
    "\n",
    "    # Connecting to Database    \n",
    "    tmp_filter = database\n",
    "\n",
    "\n",
    "    # limiting the Data by Frist Criteria and Kewords\n",
    "    both_keyword_category= [bool(keywords) == True, bool(category)==True]\n",
    "    only_keyword= [bool(keywords) == True, bool(category)==False]\n",
    "    only_category= [bool(keywords) == False, bool(category)==True]\n",
    "    all_keywords = r'\\b(?:{})\\b'.format('|'.join(map(re.escape, keywords.split(\" \"))))\n",
    "    cols_return = ['title','content','filename']\n",
    "    \n",
    "    if all(both_keyword_category):\n",
    "        tmp_criteria= [str('category_')+str(name) for name in category.split(\" \")]\n",
    "        tmp_filter =  tmp_filter[(tmp_filter[tmp_criteria]==1).any(axis=1)]\n",
    "        tmp_filter =  tmp_filter[tmp_filter.content_t.str.contains(all_keywords)]\n",
    "        return tmp_filter[cols_return] # type: ignore\n",
    "\n",
    "    elif all(only_keyword):\n",
    "        tmp_filter =  tmp_filter[tmp_filter.content_t.str.contains(all_keywords)]\n",
    "        return tmp_filter[cols_return] # type: ignore\n",
    "\n",
    "    elif all(only_category):\n",
    "        tmp_criteria= [str('category_')+str(name) for name in category.split(\" \")]\n",
    "        tmp_filter =  tmp_filter[(tmp_filter[tmp_criteria]==1).any(axis=1)]\n",
    "        return tmp_filter[cols_return]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#####  Searching by words frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Importing Stopwords \n",
    "Stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "min_words = 5\n",
    "max_words = 300\n",
    "\n",
    "def tokenizer_ (sentence:str,\n",
    "                stopwords=Stop_words,\n",
    "                lemmatize=True,\n",
    "                min_words:int = min_words ,\n",
    "                max_words:int = max_words):\n",
    "\n",
    "    \"\"\" Lemmatize, tokenize, and remove stop words\"\"\"\n",
    "    \n",
    "    if lemmatize:\n",
    "        # Initiate lemmatization\n",
    "        stemmer = nltk.stem.WordNetLemmatizer()\n",
    "        \n",
    "        # Initiate Tokenizer\n",
    "        tokens = [stemmer.lemmatize(word) for word in nltk.tokenize.word_tokenize(sentence)]\n",
    "    \n",
    "    else:\n",
    "        tokens = [ word for word in nltk.tokenize.word_tokenize(sentence)]\n",
    "\n",
    "        \n",
    "    token = [word for word in tokens if (len(word)>min_words) and (len(word)<max_words)and (word not in stopwords)]\n",
    "    \n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225, 22668)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing corpus\n",
    "token_stop = tokenizer_(' '.join(Stop_words), lemmatize=False)\n",
    "\n",
    "Vec = (sklearn.feature_extraction.text.\n",
    "       TfidfVectorizer(stop_words = Stop_words,\n",
    "                       tokenizer=tokenizer_))\n",
    "\n",
    "Vec_matrix = Vec.fit_transform(database['content'].values)\n",
    "Vec_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def search_bar_2 (sentence:str)-> pandas.DataFrame:\n",
    "    \"\"\"Return the data base sentences with cosine similarity\"\"\"\n",
    "    \n",
    "    # Transform sentences\n",
    "    tokens = [str(token) for token in tokenizer_(sentence)]\n",
    "    vector = Vec.transform(tokens)\n",
    "    \n",
    "    # Matric with similarity between query and databse\n",
    "    tmp_mat = sklearn.metrics.pairwise.cosine_similarity(vector,Vec_matrix)\n",
    "    \n",
    "    # Best cosine distance for each token \n",
    "    best_index = extract_best_indices(tmp_mat, topk=3)\n",
    "    \n",
    "    return database.iloc[best_index, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " UK Foreign Secretary Jack Straw has defended plans to end the European Union's arms embargo on China, despite opposition from the US and Japan.  Mr Straw, visiting Beijing, noted arms embargoes applied to China, Burma and Zimbabwe but not to North Korea, which he said had a terrible rights record. The EU imposed its arms ban on China in 1989 after troops opened fire on protestors in Tiananmen Square. Mr Straw also signed a deal on China-UK tourism. It is expected this would increase the number of Chinese tourists by 40,000 per year, providing $120m in revenue. China has in the past said it sees the weapons ban as politically driven, and does not want it lifted in order to buy more weapons. Mr Straw, speaking at a joint news conference with Chinese Foreign Minister Li Zhaoxing, stressed this point. \"The result of any decision [to lift the arms embargo] should not be an increase in arms exports from European Union member states to China, either in quantitative or qualitative terms,\" Mr Straw said. Earlier this week he said he expected the embargo to be lifted within six months.  But Mr Straw faces tough opposition to the move. Tory foreign affairs spokesman Michael Ancram said lifting the arms embargo would be \"irresponsible\" and would damage Britain's relations with the US. He said Mr Straw was \"naive beyond belief\" if he accepted China's claim it does not want the ban lifted in order to buy weapons. The French want the embargo lifted because they want to sell arms to China; the Chinese want it lifted because they want to buy arms and battlefield technology from Europe.\" When he was in Tokyo earlier this week, Japanese Foreign Minister Nobutaka Machimura told the British minister that his plan to remove the embargo was \" a worrying issue that concerns the security and environment of not only Japan, but also East Asia overall\". Washington argues that if the embargo is lifted, it could lead to a buying spree for arms that China could use to threaten its diplomatic rival Taiwan. Beijing says Taiwan is part of Chinese territory and wants to unite it with the mainland, by force if necessary. The US is bound by law to help Taiwan defend itself. Washington has also voiced concern that the human rights conditions in China have not improved enough to merit an end to the embargo. It is an issue raised by human rights groups too. Brad Adams, from the UK's Human Rights Watch, said: \"This is a huge political signal from Europe that they are willing to forget about Tiananmen Square.\" But Mr Straw insisted the EU's code of conduct on arms exports meant tough criteria on human rights still had to be met if the embargo was lifted. \n",
      "\n",
      "\n",
      " ----------------------- \n",
      "\n",
      " Eight out of 10 voters do not trust politicians to tell the truth, a new poll conducted for the BBC suggests.  And 87% of the 1,000 adults quizzed by ICM for BBC News 24 said politicians did not deliver what they promised. The poll comes after Foreign Secretary Jack Straw predicted trust would be \"the key choice\" at the next election. Both the Tories and the Lib Dems are keen to emphasise a perceived lack of trust in Tony Blair, following his claims over Iraqi weapons.  But according to the BBC poll, 61% said the issue of trust made no difference to whether or not they would vote at the next election, widely expected on 5 May. The poll also looked at what lay behind the lack of trust in politicians. Some 87% said politicians did not keep the promises they made before elections, while 92% said they never gave \"a straight answer\". Just under three-quarters of respondents (73%) said politicians had shown themselves to be dishonest too often.  Mr Straw told activists in Blackburn on Thursday that voters would have to decide at the next election which party \"best deserves\" their \"future trust\". \"That in the end is the key choice at the next election.\"  He acknowledged that the public had lost faith in Labour, but suggested it could persuade people to \"reinvest their trust with us\" if the party could overcome Tory attempts to spread cynicism in politics. The Conservatives are keen to highlight the trust issue. During his response to Gordon Brown's Budget statement on Tuesday, Michael Howard compared the chancellor's figures to the prime minister's claims about Iraq's weapons of mass destruction.  The Lib Dems are also keen to highlight the trust issue, with Charles Kennedy has claiming voters had a \"fundamental lack of trust in the prime minister\". And the Green Party unveiled a billboard opposite the Palace of Westminster accusing the government of lying over the Iraq war.  Former education secretary Estelle Morris told BBC News 24 that there was a \"real problem of trust\" between the public and the politicians. She said she did not feel her own colleagues could be trusted, but suggested the \"three-cornered relationship\" between the press, politicians and the public had a hand in the issue. The public was often turned off by sitting on the sidelines in \"the battle of words\" between the politician and the journalist, she added. Lib Dem foreign affairs spokesman Menzies Campbell said the Iraq war had hit trust in politicians hard. \"Issues of war and peace, life and death do have a very damaging effect on the credibility of politicians\". Martin Bell, who won the Tatton seat from Tory Neil Hamilton on anti-corruption platform, said politicians often failed to see themselves as others did. \"We need public figures we trust to tell the truth and who can see themselves as others see them.\" \n",
      "\n",
      "\n",
      " ----------------------- \n",
      "\n",
      " Billions of pounds spent on conflict in Iraq and in the Middle East should have been used to reduce poverty, Cardinal Cormac Murphy-O'Connor has said.  The head of the Catholic Church in England and Wales made the comments on BBC Radio 4 and will re-iterate his stance in his Christmas Midnight Mass. The cardinal used a Christmas message to denounce the war in Iraq as a \"terrible\" waste of money. He and the Archbishop of Canterbury have both spoken out about the war.  Speaking on BBC Radio 4's Thought for the Day slot, he criticised the fact that \"billions\" have been spent on war, instead of being used to bring people \"out of dire poverty and malnourishment and disease\". The cardinal said 2005 should be the year for campaigning to \"make history poverty\". He added: \"If the governments of the rich countries were as ready to devote to peace the resources they are willing to commit to war, that would be to see with new eyes and speak with a new voice and perhaps then others would listen to us with new ears.\" The cardinal will touch on this theme again on Friday night when he will tell the congregation of 2,000 at Westminster Cathedral that peace is \"worth, always, striving for\".  \"How is it that peace has not arrived?,\" the cardinal will ask. \"How is it that there is war in Iraq, violence in the Holy Land, and the horror of pain and death amongst the poor and deprived who suffer from injustice and thus do not find peace?\" \"How can one wish a happy Christmas for our fellow Christians in Iraq or in the Holy Land or those who suffer in Africa unless you and I, in whatever way is open to us, say and do what makes for peace?\" Both the Cardinal and Archbishop of Canterbury Dr Rowan Williams appealed for the weapons inspectors to be given more time in Iraq before the war started. Dr Williams has since criticised the government over its case for war, saying the failure to find weapons of mass destruction had damaged faith in the political system.  On Friday, the Cardinal will ask the congregation to search for peace. \"It is possible, it is real, it is worth, always, striving for, because of the promise of Our Saviour,\" he will say. \"I also wish you peace in your homes because peace in your home is the beginning of peace in the homes of the community. \" A spokesman said Downing Street had no comment to make. But Prime Minister Tony Blair has said he will put Africa at the top of the agenda when Britain chairs the G8 summit next year. \n",
      "\n",
      "\n",
      " ----------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = 'a new about weapons'\n",
    "result = search_bar_2 (sentence)\n",
    "\n",
    "for res in result:\n",
    "    print(res)\n",
    "    print()\n",
    "    print(\"\\n ----------------------- \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#####  Searching by Meaning\n",
    "\n",
    "-Semantic search seeks to improve search accuracy by understanding the content of the search query. In contrast to traditional search engines which only find documents based on lexical matches, semantic search can also find synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import tensorflow \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SentenceTransformer('all-MiniLM-L6-v2',device ='cpu')\n",
    "# model.save(r'C:\\Users\\mgarcia4\\Desktop\\Logitec-Search-Engine\\search_engine\\search_engines\\trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process keywords \n",
    "class WordEmbedding_Transformer (sklearn.base.BaseEstimator, \n",
    "                        sklearn.base.TransformerMixin):\n",
    "    \n",
    "    \"\"\"download all-MiniLM-L6-v2\"\"\"\n",
    "    \n",
    "    def __init__(self,pre_train_model_path:str) :\n",
    "        super().__init__()\n",
    "    \n",
    "        self.pre_train_model_path = pre_train_model_path\n",
    "        self.model_ = SentenceTransformer(self.pre_train_model_path,device ='cpu')\n",
    "            \n",
    "    def fit (self, X :numpy.array , y:numpy.array = None):\n",
    "        return self \n",
    "    \n",
    "    def transform(self,X :numpy.array) -> tensorflow.Tensor:\n",
    "        X = self.model_.encode(X, convert_to_tensor = True)\n",
    "        return X\n",
    "\n",
    "class WordEmbedding_Comparing (sklearn.base.BaseEstimator, \n",
    "                            sklearn.base.TransformerMixin):\n",
    "    def __init__(self,corpus:str) :\n",
    "        super().__init__()\n",
    "        self.corpus = corpus\n",
    "\n",
    "    def fit(self,X:numpy.array, y:numpy.array = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X :numpy.array) -> tensorflow.Tensor:\n",
    "        cosine_score = util.pytorch_cos_sim(X,self.corpus)[0]\n",
    "        N_top_results = torch.topk(cosine_score, k=5)\n",
    "        return N_top_results\n",
    "\n",
    "class Top_Results (sklearn.base.BaseEstimator, \n",
    "                            sklearn.base.TransformerMixin):\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        self.top_n = []\n",
    "\n",
    "    def fit(self,X:numpy.array, y:numpy.array = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X :numpy.array):\n",
    "        for score, idx in zip(X[0], X[1]):\n",
    "            score = score.cpu().data.numpy() \n",
    "            idx = self.top_n .append(idx.cpu().data.numpy())\n",
    "            \n",
    "        return self.top_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enconding the Whole data Set\n",
    "path = r'C:\\Users\\mgarcia4\\Desktop\\Logitec-Search-Engine\\search_engine\\search_engines\\trained\\model'\n",
    "embedding_corpus = WordEmbedding_Transformer(path).fit_transform(database.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'search_engines'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mgarcia4\\Desktop\\Logitec-Search-Engine\\search_engine\\Jupyter_Notebooks\\Search Engines.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mgarcia4/Desktop/Logitec-Search-Engine/search_engine/Jupyter_Notebooks/Search%20Engines.ipynb#ch0000019?line=0'>1</a>\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mUsers\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mmgarcia4\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mDesktop\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mLogitec-Search-Engine\u001b[39m\u001b[39m\\\u001b[39m\u001b[39msearch_engine\u001b[39m\u001b[39m\\\u001b[39m\u001b[39msearch_engines\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mtrained\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mFIT_BBC_NEWS_2200_CORPUS_1.pkl\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mgarcia4/Desktop/Logitec-Search-Engine/search_engine/Jupyter_Notebooks/Search%20Engines.ipynb#ch0000019?line=2'>3</a>\u001b[0m test \u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39;49mload(path)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:587\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    581\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fobj, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    582\u001b[0m                 \u001b[39m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    583\u001b[0m                 \u001b[39m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    584\u001b[0m                 \u001b[39m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n\u001b[0;32m    585\u001b[0m                 \u001b[39mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[1;32m--> 587\u001b[0m             obj \u001b[39m=\u001b[39m _unpickle(fobj, filename, mmap_mode)\n\u001b[0;32m    588\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:506\u001b[0m, in \u001b[0;36m_unpickle\u001b[1;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[0;32m    504\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 506\u001b[0m     obj \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m    507\u001b[0m     \u001b[39mif\u001b[39;00m unpickler\u001b[39m.\u001b[39mcompat_mode:\n\u001b[0;32m    508\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mThe file \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m has been generated with a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    509\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mjoblib version less than 0.10. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    510\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mPlease regenerate this pickle file.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    511\u001b[0m                       \u001b[39m%\u001b[39m filename,\n\u001b[0;32m    512\u001b[0m                       \u001b[39mDeprecationWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\pickle.py:1212\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1210\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mEOFError\u001b[39;00m\n\u001b[0;32m   1211\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(key, bytes_types)\n\u001b[1;32m-> 1212\u001b[0m         dispatch[key[\u001b[39m0\u001b[39;49m]](\u001b[39mself\u001b[39;49m)\n\u001b[0;32m   1213\u001b[0m \u001b[39mexcept\u001b[39;00m _Stop \u001b[39mas\u001b[39;00m stopinst:\n\u001b[0;32m   1214\u001b[0m     \u001b[39mreturn\u001b[39;00m stopinst\u001b[39m.\u001b[39mvalue\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\pickle.py:1528\u001b[0m, in \u001b[0;36m_Unpickler.load_global\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1526\u001b[0m module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1527\u001b[0m name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1528\u001b[0m klass \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind_class(module, name)\n\u001b[0;32m   1529\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mappend(klass)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\pickle.py:1579\u001b[0m, in \u001b[0;36m_Unpickler.find_class\u001b[1;34m(self, module, name)\u001b[0m\n\u001b[0;32m   1577\u001b[0m     \u001b[39melif\u001b[39;00m module \u001b[39min\u001b[39;00m _compat_pickle\u001b[39m.\u001b[39mIMPORT_MAPPING:\n\u001b[0;32m   1578\u001b[0m         module \u001b[39m=\u001b[39m _compat_pickle\u001b[39m.\u001b[39mIMPORT_MAPPING[module]\n\u001b[1;32m-> 1579\u001b[0m \u001b[39m__import__\u001b[39;49m(module, level\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m   1580\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[0;32m   1581\u001b[0m     \u001b[39mreturn\u001b[39;00m _getattribute(sys\u001b[39m.\u001b[39mmodules[module], name)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'search_engines'"
     ]
    }
   ],
   "source": [
    "path = r'C:\\Users\\mgarcia4\\Desktop\\Logitec-Search-Engine\\search_engine\\search_engines\\trained\\FIT_BBC_NEWS_2200_CORPUS_1.pkl'\n",
    "\n",
    "test = joblib.load(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pipeline_2 = sklearn.pipeline.Pipeline([\n",
    "    # === Transforming ===\n",
    "    (\"Emebdding Query\",\n",
    "     WordEmbedding_Transformer(path)),\n",
    "\n",
    "    (\"Comparing\",\n",
    "     WordEmbedding_Comparing(embedding_corpus)),\n",
    "\n",
    "    (\"Top_N\",\n",
    "    Top_Results())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_meaning(indexes):\n",
    "    return database.iloc[indexes][['title','content']] # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>China keeps tight rein on credit</td>\n",
       "      <td>China's efforts to stop the economy from over...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>China 'to overtake US net use'</td>\n",
       "      <td>The Chinese net-using population looks set to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2207</th>\n",
       "      <td>China 'ripe' for media explosion</td>\n",
       "      <td>Asia is set to drive global media growth to 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Millions go missing at China bank</td>\n",
       "      <td>Two senior officials at one of China's top co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>China suspends 26 power projects</td>\n",
       "      <td>China has ordered a halt to construction work...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  title  \\\n",
       "15     China keeps tight rein on credit   \n",
       "1897     China 'to overtake US net use'   \n",
       "2207   China 'ripe' for media explosion   \n",
       "497   Millions go missing at China bank   \n",
       "471    China suspends 26 power projects   \n",
       "\n",
       "                                                content  \n",
       "15     China's efforts to stop the economy from over...  \n",
       "1897   The Chinese net-using population looks set to...  \n",
       "2207   Asia is set to drive global media growth to 2...  \n",
       "497    Two senior officials at one of China's top co...  \n",
       "471    China has ordered a halt to construction work...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = Pipeline_2.fit_transform(\"china has many problems\")\n",
    "search_by_meaning(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
